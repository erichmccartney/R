setwd("~/GitHub/R/DataSets")
library("readxl")
d <- read_excel("BigMartSales.xlsx", sheet="Data")
str(d)
col <- c("Item_ID", "Item_Fat_Content", "Item_Type", "Outlet_ID", "Outlet_Size",
"City_Type", "Outlet_Type")
d[col] <- lapply(d[col], factor)
str(d)
d$Outlet_Type <- relevel(d$Outlet_Type, ref="Grocery Store")     # Smallest store
d$City_Type <- relevel(d$City_Type, ref="Tier 1")                # Largest city
colSums(is.na(d))                                                # Missing values
#' Note: 2410 missing values under Outlet_Size, 1463 missing values for Item-Weight.
#'       Outlet_Size is a proxy for Outlet_ID, we will drop Outlet_Size from our analysis.
#'       But since item price is a function of item weight, we have to keep weight in our models.
#' Data Exploration
hist(d$Item_Sales)
hist(log(d$Item_Sales))
library(lattice)
histogram(~Item_Sales | Outlet_Type, data=d)
densityplot(~Item_Sales | Outlet_Type, data=d)
bwplot(Item_Sales ~ Outlet_Type, data=d)
bwplot(Item_Sales ~ City_Type, data=d)
table(d$Outlet_Type)
table(d$City_Type)
table(d$City_Type, d$Outlet_Type)
df <- d[, c(2, 4, 6, 12)]
str(df)
library(PerformanceAnalytics)
chart.Correlation(df)                                 # Note: Only item-level variables
#' Variable selection for Mixed Level Analysis
#' Dependent variable: Item_Sales
#' Lower (item) level variables that affect item sales:
#'   Item_ID:          Unit of analysis (will not be included in model)
#'   Item_Visibility:  More visible products should sell more
#'   Item_Type:        Certain item types such as dairy or vegetables may sell more
#'   Item_MRP:         Pricier items may sell less
#' Upper (store) level Variables that affect item sales
#'   Outlet_ID:        Unit of analysis for upper-level; needed for Question 3
#'   Outlet_Type:      For Question 1: how sales vary by outlet type
#'   City_Type:        For Question 2: how sales vary by city type
#'   Outlet_Age:       New variable: 2013 - Outlet_Year, since older outlets may have
#'                     more established clientale and may sell more than new ones.
d$Outlet_Age = 2013 - d$Outlet_Year
ols <- lm(log(Item_Sales) ~ Item_Visibility + Item_Type + Item_MRP + Outlet_Type +
City_Type + Outlet_Age, data=d)       # Note: May have correlated errors
fe  <- lm(log(Item_Sales) ~ Item_Visibility + Item_Type + Item_MRP + Outlet_Type +
City_Type + Outlet_Age + Outlet_ID, data=d)
library(lme4)
re <- lmer(log(Item_Sales) ~ Item_Visibility + Item_Type + Item_MRP + Outlet_Type +
City_Type + Outlet_Age + (1 | Outlet_ID), data=d, REML=FALSE)
library(stargazer)
stargazer(ols, fe, re, type="text", single.row=TRUE)
ranef(re)
AIC(ols, fe, re)
BIC(ols, fe, re)                                      #' RE model has the best fit
library(car)
vif(re)
#' install.packages("ivpanel")
library(ivpanel)
hausman(fe, re)                                       # Hausman test: FE is better
setwd("~/GitHub/R/DataSets")
df <- read.csv("FraminghamCHD.csv")
df <- read.csv("FraminghamCHD.csv")
View(df)
dim(df)
str(df)
# df$TenYearCHD <- factor(df$TenYearCHD, labels=c("no", "yes"))
df$gender <- factor(df$male, labels=c("female", "male"))
df$currentSmoker <- factor(df$currentSmoker, labels=c("no", "yes"))
df$BPMeds <- factor(df$BPMeds, labels=c("no", "yes"))
df$prevalentStroke <- factor(df$prevalentStroke, labels=c("no", "yes"))
df$prevalentHyp <- factor(df$prevalentHyp, labels=c("no", "yes"))
df$diabetes <- factor(df$diabetes, labels=c("no", "yes"))
df$education <- NULL                                      # Drop education column
colSums(is.na(df))
df <- df[complete.cases(df), ]                            # Drop incomplete rows
str(df)
table(df$TenYearCHD)                                      # Unbalanced sample
table(df$TenYearCHD, df$gender)
plot(TenYearCHD ~ age, data=df)
plot(TenYearCHD ~ gender, data=df)
plot(TenYearCHD ~ totChol, data=df)
linear <- lm(TenYearCHD ~ age + gender + cigsPerDay + BPMeds + prevalentStroke + prevalentHyp +
totChol + sysBP + diaBP + BMI + heartRate + glucose, data=df)
summary(linear)
plot(linear)                                              # Fails normality and homoskedasticity
logit  <- glm(TenYearCHD ~ age + gender + cigsPerDay + BPMeds + prevalentStroke + prevalentHyp +
totChol + sysBP + diaBP + BMI + heartRate + glucose, family=binomial (link="logit"), data=df)
summary(logit)
probit <- glm(TenYearCHD ~ age + gender + cigsPerDay + BPMeds + prevalentStroke + prevalentHyp +
totChol + sysBP + diaBP + BMI + heartRate + glucose, family=binomial (link="probit"), data=df)
summary(probit)
library(stargazer)
stargazer(df, title="Descriptive Statistics", type="text")
stargazer(linear, logit, probit, title="10 Year CHD Classification", type="text")
AIC(linear)
logit$coef                                                # Log odds ratio
probit$coef
confint(logit)                                            # 95% confidence interval (using log-likelihood)
confint.default(logit)                                    # 95% confidence interval (using standard errors)
exp(logit$coef)                                           # Odds ratio
exp(cbind(OddsRatio = coef(logit), confint(logit)))       # Odds Ratio and 95% C.I.
#' Predicted probabilities
predols <- predict(linear, type="response")
summary(predols)
predlogit <- predict(logit, type="response")              # Predicted values of y (in the scale of response variable, i.e. log-odds)
summary(predlogit)
predprobit <-predict(probit, type="response")
summary(predprobit)
#' Marginal effects estimation
head(predict(logit, type="link"))                         # Predicted values of y (in the scale of linear predictors)
dlogis(predict(logit, type="link"))                       # Density function for the logistic distribution of y
LogitScalar <- mean(dlogis(predict(logit, type="link")))  # Mean of density function of y
LogitScalar*coef(logit)                                   # Marginal effects
ProbitScalar <- mean(dnorm(predict(probit, type="link")))
ProbitScalar*coef(probit)
# Accuracy estimation: Confusion matrix
table(df$TenYearCHD, round(fitted(logit)))
table(df$TenYearCHD, round(fitted(probit)))
# McFadden's Pseudo R-squared
library(DescTools)
PseudoR2(logit)
PseudoR2(logit, c("McFadden", "Nagel"))
# Comparing models: Likelihood-ratio test
library(lmtest)
lrtest(logit, linear)                                    # Can't compare OLS vs GLM models
null  <- glm(TenYearCHD ~ 1, family=binomial (link="logit"), data=df)
lrtest(null, logit)
lrtest(logit, probit)
# install.packages("rcompanion")
library(rcompanion)
nagelkerke(logit)
#' Accuracy estimation using training and test datasets
set.seed(1234)
trainIndex = sample(1:nrow(df), size=round(0.75*nrow(df)), replace=FALSE)
train <- df[trainIndex,]
test  <- df[-trainIndex,]
dim(train); dim(test)
library(DescTools)
PseudoR2(logit)
predlogit <- predict(logit, type="response")              # Predicted values of y (in the scale of response variable, i.e. log-odds)
summary(predlogit)
set.seed(1234)
trainIndex = sample(1:nrow(df), size=round(0.75*nrow(df)), replace=FALSE)
train <- df[trainIndex,]
test  <- df[-trainIndex,]
dim(train); dim(test)
logit  <- glm(TenYearCHD ~ age + male + cigsPerDay + BPMeds + prevalentStroke + prevalentHyp +
totChol + sysBP + diaBP + BMI + heartRate + glucose, family=binomial (link="logit"), data=train)
test_x <- test[ , c(1:14)]
predlogit <-predict(logit, newdata=test_x, type="response")
predlogit <- ifelse(predlogit>0.5, 1, 0)
predlogit
table(test$TenYearCHD, predlogit)                         # Confusion matrix
# install.packages("ROCR")
library(ROCR)
pr <- prediction(predlogit, test$TenYearCHD)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)                                                 # ROC plot: TP vs FP
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
setwd("~/GitHub/R/DataSets")
d <- read.csv("CocaCola.csv")
View(d)
str(d)
n <- nrow(d)
n
d$Time <- seq(1:n)
d$Time
m1 <- lm(Sales ~ Time, data=d)
summary(m1)
plot(Sales ~ Time, data=d)
abline(m1, col="red")
plot(m1)
plot(m1$res ~ d$t)
d$Qtr <- rep(1:4, n/4)
d$Qtr
d$Qtr <- factor(d$Qtr)
d$Qtr
d$Qtr <- relevel(d$Qtr, 4)
d$Qtr
View(d)
m2 <- lm(Sales ~ Time + Qtr, data=d)
summary(m2)
library(stargazer)
stargazer(m1, m2, type="text", single.row=TRUE)
train <- d[1:40, ]
test  <- d[41:56, ]
m1a <- lm(Sales ~ Time, data=train)
m2a <- lm(Sales ~ Time + Qtr, data=train)
pred_test_m1a <- predict(m1, test)
pred_test_m2a <- predict(m2, test)
sqrt(mean((test$Sales - pred_test_m1a) ^ 2))
sqrt(mean((test$Sales - pred_test_m2a) ^ 2))
library(lmtest)
dwtest(m1)
dwtest(m2)
acf(d$Sales)
pacf(d$Sales)
d$SalesLag1 <- c(NA, d$Sales[1:n-1])
d$SalesLag2 <- c(NA, NA, d$Sales[1:54])
d$SalesLag3 <- c(NA, NA, NA, d$Sales[1:53])
View(d)
train <- d[1:40, ]
test  <- d[41:56, ]
m3 <- lm(Sales ~ Time + Qtr + SalesLag1, data=train)
summary(m3)
m4 <- lm(Sales ~ Time + Qtr + SalesLag1 + SalesLag2 + SalesLag3, data=train)
summary(m4)
stargazer(m1, m2, m3, m4, type="text", single.row=TRUE)
sqrt(mean((test$Sales - predict(m1, test)) ^ 2))
sqrt(mean((test$Sales - predict(m2, test)) ^ 2))
sqrt(mean((test$Sales - predict(m3, test)) ^ 2))
sqrt(mean((test$Sales - predict(m4, test)) ^ 2))
data(AirPassengers)
ts <- AirPassengers
ts
View(ts)
str(ts)
# Explore data
start(ts)
View(ts)
ts
str(ts)
# Explore data
start(ts)
end(ts)
class(ts)
frequency(ts)
cycle(ts)
# Visualize data
plot(ts, xlab="Month/Year", ylab="Passenger Count (1000's)", main="Air Passenger Counts")
abline(lm(ts ~ time(ts)), col="red")  # Linear trend plot
boxplot(ts ~ cycle(ts))               # Seasonality plot
# Identification: Identify d in I(d)
plot(log(ts))                      # Log takes care of unequal variance
boxplot(ts ~ cycle(ts))               # Seasonality plot
# Identification: Identify d in I(d)
plot(log(ts))                      # Log takes care of unequal variance
plot(diff(log(ts)))                # Differentiating makes the mean zero; hence d=1
# install.packages("tseries")
library(tseries)
adf.test(diff(log(ts)))            # Augmented Dickey-Fuller Test, H0: Time series is not stationary
# Estimation: Estimate p and q for AR(p) and MA(q)
acf(ts)                            # Correlation between Y(t) and lags Y(t-p)
# Dashed blue line indicates 95% C.I. for stationary series
# Our goal is to bring the ACFs within the blue lines
acf(diff(log(ts)))                 # p = 2 for AR(p)
pacf(diff(log(ts)))                # q = 1 for MA(q)
# ARIMA model
model <- arima(log(ts), c(2,1,1), seasonal=list(order=c(2,1,1), period=12))  # c(p,d,q)
model
# Estimation: Estimate p and q for AR(p) and MA(q)
acf(ts)                            # Correlation between Y(t) and lags Y(t-p)
# Dashed blue line indicates 95% C.I. for stationary series
# Our goal is to bring the ACFs within the blue lines
acf(diff(log(ts)))                 # p = 2 for AR(p)
pacf(diff(log(ts)))                # q = 1 for MA(q)
# Dashed blue line indicates 95% C.I. for stationary series
# Our goal is to bring the ACFs within the blue lines
acf(diff(log(ts)))                 # p = 2 for AR(p)
pacf(diff(log(ts)))                # q = 1 for MA(q)
# ARIMA model
model <- arima(log(ts), c(2,1,1), seasonal=list(order=c(2,1,1), period=12))  # c(p,d,q)
model
forecasted <- predict(model, n.ahead=5*12)
forecasted
forecasted <- exp(forecasted$pred)
forecasted <- round(forecasted,0)
forecasted
forecasted <- predict(model, n.ahead=5*12)
forecasted
forecasted <- exp(forecasted$pred)
forecasted
forecasted <- round(forecasted,0)
forecasted
ts.plot(ts, forecasted, lty=c(1,3))
# Cross-validation using training data Year 1949-1958, and test data Year 1959-1960
train <- ts(ts, frequency=12, start=c(1949,1), end=c(1958,12))
test  <- tail(ts, 24)
model <- arima(log(train), c(2,1,1), seasonal=list(order=c(2,1,1), period=12))
forecasted <- predict(model, n.ahead=2*12)
forecasted <- exp(forecasted$pred)
forecasted <- round(forecasted,0)
forecasted
test
test - forecasted
n <- length(test - forecasted)
RMSE <- sqrt(sum((test - forecasted)^2)/n)
RMSE                                        # RMSE=35
# Using auto.arima() function in forecast package
# install.packages("forecast")
library(forecast)
model2 <- auto.arima(ts)
model2
forecasted2 <- forecast(model2, level=c(95), h=5*12)  # c is 95% CI and h is forecast horizon
forecasted2 <- round(forecasted2$mean,0)
forecasted2
ts.plot(ts, forecasted2, lty=c(1,3))
model2 <- auto.arima(train)
forecasted2 <- predict(model2, n.ahead=2*12)
forecasted2 <- round(forecasted2$pred,0)
forecasted2
test
test - forecasted2
RMSE2 <- sqrt(sum((test - forecasted2)^2)/n)
RMSE2                                       # RMSE2=74
